{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "training_spectrogram.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ijdXNpxuZHp7dm0zBtwiR-ocqDlg6IYg",
      "authorship_tag": "ABX9TyMMkBSWu3or+lbse2DLnm22",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/is0280fp/google_colab/blob/use_image_as_input/training_spectrogram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNwUIy5fsXtu",
        "outputId": "1edc027d-0e85-42fe-eb61-a6049238654b"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx9ajZC-pFYu"
      },
      "source": [
        "# ラベルの読み込み\n",
        "import pickle\n",
        "with open(\"/content/drive/MyDrive/pickle/label_five_class.pickle\", mode=\"rb\") as f:\n",
        "   label = pickle.load(f)\n",
        "f.close()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBQp7qm8Z4Jw",
        "outputId": "cef4639a-5549-451f-d9d0-928af1b12fc2"
      },
      "source": [
        "# データセット内訳チェック\n",
        "import numpy as np\n",
        "\n",
        "print(np.array(np.where(label == 0)).shape)\n",
        "print(np.array(np.where(label == 1)).shape)\n",
        "print(np.array(np.where(label == 2)).shape)\n",
        "print(np.array(np.where(label == 3)).shape)\n",
        "print(np.array(np.where(label == 4)).shape)\n",
        "\n",
        "# NaNチェック\n",
        "print(np.isnan(label).sum())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 7755)\n",
            "(1, 12748)\n",
            "(1, 19873)\n",
            "(1, 21386)\n",
            "(1, 13970)\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-KPR8cl7-ul",
        "outputId": "1224b088-7a5a-495d-d82e-9ed1b2f19284"
      },
      "source": [
        "label.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75732,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdXgcprG6dXD",
        "outputId": "ad2a6cd3-29fc-495c-a383-b1cb22a7e6d0"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers, regularizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, LSTM, InputLayer, Dense, Dropout, Activation, Flatten, concatenate, Conv1D, MaxPooling1D, Input, Reshape, Bidirectional, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.callbacks import TensorBoard, LearningRateScheduler\n",
        "import time\n",
        "import math\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# NAME = \"HC-vs-C-LSTM_2class\"\n",
        "# NAME = \"HC-vs-A-LSTM_2class\"\n",
        "# NAME = \"HC-vs-K-LSTM_2class\"\n",
        "# NAME = \"HC-vs-H-LSTM_2class\"\n",
        "# NAME = \"HC-vs-GDs-LSTM_2class\"\n",
        "# NAME = \"H-vs-K-vs-A-vs-C-LSTM_4class\"\n",
        "#NAME = \"HC-vs-H-vs-K-vs-A-vs-C-LSTM_5class\"\n",
        "# NAME = \"H-vs-K-vs-A-vs-C-CNN_4class\"\n",
        "NAME = \"HC-vs-Patients-CNN_5class\"\n",
        "# NAME = \"HC-vs-GDs-CNN_2class\"\n",
        "# NAME = \"HC-vs-H-CNN_2class\"\n",
        "# NAME = \"HC-vs-K-CNN_2class\"\n",
        "# NAME = \"HC-vs-A-CNN_2class\"\n",
        "# NAME = \"HC-vs-C-CNN_2class\"\n",
        "\n",
        "#-----------------------------------初期設定-------------------------------------------\n",
        "# tensorboardのログ\n",
        "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
        "\n",
        "# 学習打ち切り目安\n",
        "early_stopping = EarlyStopping(\n",
        "                        monitor='val_loss',\n",
        "                        min_delta=0.0,\n",
        "                        patience=10,\n",
        "                )\n",
        "\n",
        "# checkpointの設定\n",
        "checkpoint = ModelCheckpoint(\n",
        "                    filepath=\"saved_model/{}\".format(NAME),\n",
        "                    monitor='val_loss',\n",
        "                    save_best_only=True,\n",
        "                    period=1,\n",
        "                )\n",
        "\n",
        "# weightの初期化\n",
        "# initializer = tf.initializers.he_normal()\n",
        "\n",
        "#-----------------------------------学習モデル作成-------------------------------------------\n",
        "\n",
        "\n",
        "#-----------------------------------Optimizer-------------------------------------------\n",
        "# Optimizer: 多クラス分類+Adam\n",
        "# model.compile(loss='sparse_categorical_crossentropy', \n",
        "#               optimizer= optimizers.Adam(lr=0.002, beta_1= 0.9, beta_2= 0.999),\n",
        "#               metrics=['accuracy'],\n",
        "#               )\n",
        "\n",
        "# Optimizer: 多クラス分類+SGD\n",
        "# model.compile(loss='sparse_categorical_crossentropy',\n",
        "#               optimizer= optimizers.SGD(lr=5e-2, decay=1e-2),\n",
        "#               metrics=['accuracy'],\n",
        "#               )\n",
        "\n",
        "# Optimizer: 2クラス分類\n",
        "# model.compile(loss='binary_crossentropy',\n",
        "#               optimizer= optimizers.SGD(lr=5e-3),\n",
        "#               metrics=['accuracy'],\n",
        "#               )\n",
        "\n",
        "#-----------------------------------summer出力-------------------------------------------\n",
        "# model.summary()\n",
        "\n",
        "#-----------------------------------学習-------------------------------------------\n",
        "# model.fit(X, train_label,\n",
        "#           batch_size=5,\n",
        "#           epochs=1000,\n",
        "#           validation_split=0.2,\n",
        "#           callbacks=[tensorboard])\n",
        "\n",
        "# training parameters = kernel_size * (1Dだったらここは1, 2Dだったらここはkernel_size) * output_channels + num_bias\n",
        "# 216 = 8 * 1 * 24 + 24"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQdc-9W5DVLn"
      },
      "source": [
        "# 学習モデル構成\n",
        "def lenet(INPUT_SHAPE, num_classes):\n",
        "    \"\"\"\n",
        "    http://tecmemo.wpblog.jp/wp-content/uploads/2017/03/dl_lenet-01.png この表を参考に一部活性化関数を変更してLenetを定義\n",
        "    \"\"\"\n",
        "    initializer = tf.initializers.he_normal()\n",
        "    model = Sequential()\n",
        "    # フィルターを6枚用意, 小窓のサイズ5×5, paddingによって入力と出力の画像サイズは同じ\n",
        "    # from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
        "    model.add(Conv2D(\n",
        "        64, kernel_size=3, padding=\"same\",\n",
        "        input_shape=INPUT_SHAPE, activation=\"relu\",\n",
        "        kernel_initializer=initializer\n",
        "    ))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(64, kernel_size=3, strides=1, padding=\"same\",\n",
        "                     activation=\"relu\", kernel_initializer=initializer))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "    model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\",\n",
        "                     activation=\"relu\", kernel_initializer=initializer))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "    model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\",\n",
        "                     activation=\"relu\", kernel_initializer=initializer))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "    model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\",\n",
        "                     activation=\"relu\", kernel_initializer=initializer))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\",\n",
        "                     activation=\"relu\", kernel_initializer=initializer))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\",\n",
        "                     activation=\"relu\", kernel_initializer=initializer))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "    model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\",\n",
        "                     activation=\"relu\", kernel_initializer=initializer))\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "\n",
        "    # Flatten()はマトリックスを1次元ベクトルに変換する層\n",
        "    # FCにつなぐために必要\n",
        "    model.add(Dense(num_classes, kernel_initializer=initializer))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9Jgh1RXNzjC"
      },
      "source": [
        "# 入力データの加工\n",
        "\n",
        "def conv_binary(number):\n",
        "  V_left_filename = '/content/spectrogram_data/V_left/{}.png'.format(number)\n",
        "  V_right_filename = '/content/spectrogram_data/V_right/{}.png'.format(number)\n",
        "  V_left_img = Image.open(V_left_filename)\n",
        "  V_right_img = Image.open(V_right_filename)\n",
        "  V_left_img = V_left_img.resize((256, 512))\n",
        "  V_right_img = V_right_img.resize((256, 512))\n",
        "  # 色の変換も簡単ですが、できる色が制限されます。\n",
        "  # print(img.mode) # RGBA\n",
        "  V_left_rgb = np.array(V_left_img.convert('RGB'))\n",
        "  V_right_rgb = np.array(V_right_img.convert('RGB'))\n",
        "  # numpy配列の取得\n",
        "  img_array = np.concatenate([V_left_rgb, V_right_rgb], axis=1)\n",
        "  return img_array"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7LsTj2H7S68"
      },
      "source": [
        "# acquire the .csv name\n",
        "TRAINS = range(len(label))\n",
        "y = label\n",
        "# split test and train\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    TRAINS, y, test_size=0.1, random_state=42)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hdvubAwGfJW"
      },
      "source": [
        "def get_batch(batch_size, X, Y):\n",
        "    \"\"\"\n",
        "    batchを取得する関数\n",
        "    \"\"\"\n",
        "    SIZE = len(X)\n",
        "    # n_batchs\n",
        "    n_batchs = SIZE//batch_size\n",
        "    # for でyield\n",
        "    i = 0\n",
        "    while (i < n_batchs):\n",
        "        # print(\"doing\", i, \"/\", n_batchs)\n",
        "        # Y_batch = Y[(i * n_batchs):(i * n_batchs + batch_size)]\n",
        "        Y_batch = Y[i:(i + batch_size)]\n",
        "        \n",
        "        #あるbatchのfilenameの配列を持っておく\n",
        "        X_batch_name = X[i:(i + batch_size)]\n",
        "\n",
        "        # filenameにしたがってバッチのtensorを構築\n",
        "        X_batch = np.array([conv_binary(file) for file in X_batch_name])\n",
        "        # from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
        "        # これで(batch_size, 28, 28, 1)のtrainのテンソルが作られる\n",
        "        i += 1\n",
        "        yield X_batch, Y_batch"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hpw_JfWO86am",
        "outputId": "6c8eec2d-2ee0-4d7a-b2ea-8f81f6af78e4"
      },
      "source": [
        "model = lenet((512, 512, 3), 5)\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=optimizers.Adam(lr=0.002, beta_1= 0.9, beta_2= 0.999),\n",
        "              metrics=['accuracy'])\n",
        "N_EPOCHS = 1000\n",
        "BATCH_SIZE = 64\n",
        "# for epoch in range(N_EPOCHS):\n",
        "#     print(\"=\" * 50)\n",
        "#     print(\"epoch\", epoch, \"/\", N_EPOCHS)\n",
        "#     acc = []\n",
        "    \n",
        "#     # batch_size=1000でHDDからバッチを取得する\n",
        "#     for X_batch, Y_batch in get_batch(BATCH_SIZE):\n",
        "#         # from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
        "#         model.train_on_batch(X_batch, Y_batch)\n",
        "#         score = model.evaluate(X_batch, Y_batch)\n",
        "#         print(\"batch accuracy:\", score[1])\n",
        "#         acc.append(score[1])\n",
        "#     print(\"Train accuracy\", np.mean(acc))\n",
        "#     # from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
        "#     score = model.evaluate(np.array(X_test), np.array(Y_test))\n",
        "#     print(\"Test loss:\", score[0])\n",
        "#     print(\"Test accuracy:\", score[1])\n",
        "\n",
        "training_generator = get_batch(BATCH_SIZE, X_train, Y_train)\n",
        "validation_generator = get_batch(BATCH_SIZE, X_test, Y_test)\n",
        "model.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=validation_generator,\n",
        "    steps_per_epoch=len(Y_train)//BATCH_SIZE,\n",
        "    validation_steps=len(Y_test)//BATCH_SIZE,\n",
        "    epochs=N_EPOCHS,\n",
        "    shuffle=True,\n",
        "    callbacks=[tensorboard]\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_88 (Conv2D)           (None, 512, 512, 64)      1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_77 (MaxPooling (None, 256, 256, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_89 (Conv2D)           (None, 256, 256, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_78 (MaxPooling (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_90 (Conv2D)           (None, 128, 128, 128)     73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_79 (MaxPooling (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_91 (Conv2D)           (None, 64, 64, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_80 (MaxPooling (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_92 (Conv2D)           (None, 32, 32, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_81 (MaxPooling (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_93 (Conv2D)           (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_82 (MaxPooling (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_94 (Conv2D)           (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_83 (MaxPooling (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_95 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_11  (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 5)                 2565      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 5)                 0         \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 2,475,717\n",
            "Trainable params: 2,475,717\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1064/1064 [==============================] - 1132s 1s/step - loss: 139.4950 - accuracy: 0.2953 - val_loss: 2.3524 - val_accuracy: 0.2926\n",
            "Epoch 2/1000\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1064000 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 118 batches). You may need to use the repeat() function when building your dataset.\n",
            "1064/1064 [==============================] - 0s 38us/step - loss: 22.3887 - accuracy: 0.3856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7c6a7f9048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUR9PxQaG-cr",
        "outputId": "6b6f096c-c4f8-4379-ec03-7febc22f41ce"
      },
      "source": [
        "np.array(X_test).shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7574,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3os7lDbgHEBI",
        "outputId": "02fa15f1-464b-4984-d7a4-17d74768225f"
      },
      "source": [
        "np.array(Y_test).shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7574,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1nBpLOa8dpf"
      },
      "source": [
        "# データの読み込み \n",
        "# yesを選ぶように!\n",
        "!unzip /content/drive/MyDrive/zip_spectrogram_data/AP_left.zip\n",
        "!unzip /content/drive/MyDrive/zip_spectrogram_data/AP_right.zip\n",
        "!unzip /content/drive/MyDrive/zip_spectrogram_data/AP_right_69826.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PcYi1KLxa5G"
      },
      "source": [
        "# データの読み込み\n",
        "# yesを選ぶように!\n",
        "!unzip /content/drive/MyDrive/zip_spectrogram_data/ML_left.zip\n",
        "!unzip /content/drive/MyDrive/zip_spectrogram_data/ML_left_58106.zip\n",
        "!unzip /content/drive/MyDrive/zip_spectrogram_data/ML_right.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfcILK3fxdA-"
      },
      "source": [
        "# データの読み込み\n",
        "# yesを選ぶように!\n",
        "!unzip /content/drive/MyDrive/zip_spectrogram_data/V_left.zip\n",
        "!unzip /content/drive/MyDrive/zip_spectrogram_data/V_left_48327.zip\n",
        "!unzip /content/drive/MyDrive/zip_spectrogram_data/V_right.zip\n",
        "!unzip /content/drive/MyDrive/zip_spectrogram_data/V_right_34533.zip"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}