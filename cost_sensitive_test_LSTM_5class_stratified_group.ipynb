{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cost_sensitive_test_LSTM_5class_stratified_group.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/is0280fp/google_colab/blob/main/cost_sensitive_test_LSTM_5class_stratified_group.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxh9unKPQw0O",
        "outputId": "3331ec01-71eb-4d06-a06d-40f097b4eb17"
      },
      "source": [
        "# %tensorflow_version 2.x\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_Q5Sfsrrg0D"
      },
      "source": [
        "# # StratifiedGroupKFoldの分割結果をpickle\n",
        "import pickle\n",
        "\n",
        "# test\n",
        "with open(\"/content/drive/MyDrive/test_data/V_test.pickle\", mode=\"rb\") as f:\n",
        "   test_V = pickle.load(f)\n",
        "f.close()\n",
        "with open(\"/content/drive/MyDrive/test_data/AP_test.pickle\", mode=\"rb\") as f:\n",
        "   test_AP = pickle.load(f)\n",
        "f.close()\n",
        "with open(\"/content/drive/MyDrive/test_data/ML_test.pickle\", mode=\"rb\") as f:\n",
        "   test_ML = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/test_data/COP_AP_test.pickle\", mode=\"rb\") as f:\n",
        "   test_COP_AP = pickle.load(f)\n",
        "f.close()\n",
        "with open(\"/content/drive/MyDrive/test_data/COP_ML_test.pickle\", mode=\"rb\") as f:\n",
        "   test_COP_ML = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/test_data/label_test.pickle\", mode=\"rb\") as f:\n",
        "   test_label = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/test_data/test_data_index.pickle\", mode=\"rb\") as f:\n",
        "   test_index = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "# trainig and validation\n",
        "with open(\"/content/drive/MyDrive/test_data/V_val_train.pickle\", mode=\"rb\") as f:\n",
        "   V = pickle.load(f)\n",
        "f.close()\n",
        "with open(\"/content/drive/MyDrive/test_data/AP_val_train.pickle\", mode=\"rb\") as f:\n",
        "   AP = pickle.load(f)\n",
        "f.close()\n",
        "with open(\"/content/drive/MyDrive/test_data/ML_val_train.pickle\", mode=\"rb\") as f:\n",
        "   ML = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/test_data/COP_AP_val_train.pickle\", mode=\"rb\") as f:\n",
        "   COP_AP = pickle.load(f)\n",
        "f.close()\n",
        "with open(\"/content/drive/MyDrive/test_data/COP_ML_val_train.pickle\", mode=\"rb\") as f:\n",
        "   COP_ML = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "# label\n",
        "with open(\"/content/drive/MyDrive/test_data/train_idx_list.pickle\", mode=\"rb\") as f:\n",
        "   train_index_list = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/test_data/val_idx_list.pickle\", mode=\"rb\") as f:\n",
        "   val_index_list = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/test_data/label_val_train.pickle\", mode=\"rb\") as f:\n",
        "   label = pickle.load(f)\n",
        "f.close()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyufX9F9tT3t",
        "outputId": "a8c37ef3-a414-4f01-e0fc-a99fbd68b46e"
      },
      "source": [
        "import numpy as np\n",
        "print(np.where(label==0)[0].shape)\n",
        "print(np.where(label==1)[0].shape)\n",
        "print(np.where(label==2)[0].shape)\n",
        "print(np.where(label==3)[0].shape)\n",
        "print(np.where(label==4)[0].shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13966,)\n",
            "(11969,)\n",
            "(18135,)\n",
            "(19398,)\n",
            "(14853,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHgTDcnyrjJa"
      },
      "source": [
        "# min_max normalizationする場合\n",
        "from sklearn import preprocessing\n",
        "mmscaler_LSTM = preprocessing.MinMaxScaler(feature_range=(-1, 1), copy=True) # インスタンスの作成、活性化関数でtanhを使うのでfeature_range=(-1,1) \n",
        "mmscaler_CNN = preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True) # インスタンスの作成、活性化関数でtanhを使うのでfeature_range=(-1,1)\n",
        "\n",
        "# LSTM\n",
        "mmscaler_LSTM.fit(V)\n",
        "V = mmscaler_LSTM.transform(V)\n",
        "V = V.reshape(V.shape[0], V.shape[1], 1)\n",
        "\n",
        "mmscaler_LSTM.fit(AP)\n",
        "AP = mmscaler_LSTM.transform(AP)\n",
        "AP = AP.reshape(AP.shape[0], AP.shape[1], 1)\n",
        "\n",
        "mmscaler_LSTM.fit(ML)\n",
        "ML = mmscaler_LSTM.transform(ML)\n",
        "ML = ML.reshape(ML.shape[0], ML.shape[1], 1)\n",
        "\n",
        "mmscaler_LSTM.fit(COP_AP)\n",
        "COP_AP = mmscaler_LSTM.transform(COP_AP)\n",
        "COP_AP = COP_AP.reshape(COP_AP.shape[0], COP_AP.shape[1], 1)\n",
        "\n",
        "mmscaler_LSTM.fit(COP_ML)\n",
        "COP_ML = mmscaler_LSTM.transform(COP_ML)\n",
        "COP_ML = COP_ML.reshape(COP_ML.shape[0], COP_ML.shape[1], 1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX8qUiRLcizV",
        "outputId": "fbfdf19a-00ee-41a1-a6dd-b1d26b354612"
      },
      "source": [
        "!pip install keras-layer-normalization\n",
        "# !pip install keras-self-attention"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-layer-normalization\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-layer-normalization) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-layer-normalization) (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras->keras-layer-normalization) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras->keras-layer-normalization) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras->keras-layer-normalization) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->Keras->keras-layer-normalization) (1.15.0)\n",
            "Building wheels for collected packages: keras-layer-normalization\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp37-none-any.whl size=5269 sha256=060db552ce8b8bb8f4c448ade373af6c7b605dd03f51372b61d887d9dfb66f44\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "Successfully built keras-layer-normalization\n",
            "Installing collected packages: keras-layer-normalization\n",
            "Successfully installed keras-layer-normalization-0.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkIN1zeark6f",
        "outputId": "3687af02-b3df-4ece-cbdc-ec93705047cc"
      },
      "source": [
        "from keras import optimizers, regularizers\n",
        "from keras.initializers import he_normal\n",
        "from keras.layers import ELU, Conv2D, MaxPooling2D, LSTM, InputLayer, Dense, Dropout, Activation, Flatten, concatenate, Conv1D, MaxPooling1D, Input, Reshape, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.layers.advanced_activations import PReLU\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import TensorBoard, LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras_layer_normalization import LayerNormalization\n",
        "# from keras_self_attention import SeqSelfAttention\n",
        "import numpy as np\n",
        "import keras\n",
        "from sklearn.utils import class_weight\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "\n",
        "NAME = \"COP_LSTM_5class_stratified_group_with_both\"\n",
        "i = 0\n",
        "\n",
        "# 共通\n",
        "now = datetime.datetime.now()\n",
        "\n",
        "# early stopping\n",
        "early_stopping = EarlyStopping(\n",
        "                        monitor='val_loss',\n",
        "                        min_delta=0.0,\n",
        "                        patience=10,\n",
        "                )\n",
        "\n",
        "# checkpointの設定\n",
        "checkpoint = ModelCheckpoint(\n",
        "                    filepath=\"drive/MyDrive/checkpoints/{}fold_{}.h5\".format(i, NAME),\n",
        "                    monitor='val_loss',\n",
        "                    save_best_only=True,\n",
        "                    period=1,\n",
        "                    verbose=1\n",
        "                )\n",
        "\n",
        "initializer = he_normal()\n",
        "\n",
        "classes_main = 5\n",
        "\n",
        "#LSTM-----------------------------------------------------------------------------------------------------------------------\n",
        "def build_model():\n",
        "  # LSTM\n",
        "  # 最初これだけだった\n",
        "  model = Sequential()\n",
        "  model.add(InputLayer(input_shape=(101, 5)))\n",
        "\n",
        "  model.add(Bidirectional(LSTM(512, return_sequences=True)))\n",
        "  # model.add(LSTM(256, return_sequences=True))\n",
        "  model.add(LayerNormalization())\n",
        "  model.add(Activation('tanh'))\n",
        "  model.add(Dropout(0.5))\n",
        "  \n",
        "  model.add(Flatten())\n",
        "\n",
        "  model.add(Dense(512, kernel_initializer=initializer))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Dense(256, kernel_initializer=initializer))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  # model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Dense(128, kernel_initializer=initializer))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  # model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Dense(64, kernel_initializer=initializer))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  # model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Dense(classes_main, kernel_initializer=initializer))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# k-fold目のモデルのval_accuracyとval_lossのログ\n",
        "history = []\n",
        "# 混合行列のログ\n",
        "cmx_list = []\n",
        "# F1などスコアのログ\n",
        "report_list = []\n",
        "\n",
        "# EPOCHS = 500\n",
        "\n",
        "val_label_list = []\n",
        "train_label_list = []\n",
        "\n",
        "val_LSTM_data_list = []\n",
        "train_LSTM_data_list = []\n",
        "\n",
        "i = 0\n",
        "for i in range(5):\n",
        "  train_index = train_index_list[i]\n",
        "  train_LSTM_data = np.concatenate([V[train_index], AP[train_index], ML[train_index], COP_AP[train_index], COP_ML[train_index]], axis=2)\n",
        "  val_index = val_index_list[i]\n",
        "  val_LSTM_data = np.concatenate([V[val_index], AP[val_index], ML[val_index], COP_AP[val_index], COP_ML[val_index]], axis=2)\n",
        "  \n",
        "  # label\n",
        "  train_label = label[train_index] \n",
        "  val_label= label[val_index] \n",
        "\n",
        "  # シャッフル\n",
        "  p = np.random.permutation(len(train_label))\n",
        "  train_LSTM_data = train_LSTM_data[p]\n",
        "  train_label = train_label[p]\n",
        "\n",
        "  p = np.random.permutation(len(val_label))\n",
        "  val_LSTM_data = val_LSTM_data[p]\n",
        "  val_label = val_label[p]\n",
        "\n",
        "  # MAX = len(train_LSTM_data)\n",
        "\n",
        "  if i == 0:\n",
        "    BATCH_SIZE = 55\n",
        "    # train_LSTM_data = train_LSTM_data[:-3]\n",
        "    # train_label = train_label[:-3]\n",
        "\n",
        "  elif i == 1:\n",
        "    BATCH_SIZE = 55\n",
        "    train_LSTM_data = train_LSTM_data[:-1]\n",
        "    train_label = train_label[:-1]\n",
        "\n",
        "  elif i == 2:\n",
        "    BATCH_SIZE = 55\n",
        "    train_LSTM_data = train_LSTM_data[:-1]\n",
        "    train_label = train_label[:-1]\n",
        "\n",
        "  elif i == 3:\n",
        "    BATCH_SIZE = 55\n",
        "    train_LSTM_data = train_LSTM_data[:-1]\n",
        "    train_label = train_label[:-1]\n",
        "\n",
        "  elif i == 4:\n",
        "    BATCH_SIZE = 55\n",
        "    train_LSTM_data = train_LSTM_data[:-1]\n",
        "    train_label = train_label[:-1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  train_label = keras.utils.to_categorical(train_label, classes_main)\n",
        "  val_label = keras.utils.to_categorical(val_label, classes_main)\n",
        "\n",
        "  train_LSTM_data_list.append(train_LSTM_data)\n",
        "  val_LSTM_data_list.append(val_LSTM_data)\n",
        "\n",
        "  val_label_list.append(val_label)\n",
        "  train_label_list.append(train_label)\n",
        "\n",
        "  print(\"train_data\", train_LSTM_data.shape)\n",
        "  print(\"val_data\", val_LSTM_data.shape)\n",
        "  i = i+1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "train_data (68530, 101, 5)\n",
            "val_data (9581, 101, 5)\n",
            "train_data (68530, 101, 5)\n",
            "val_data (9012, 101, 5)\n",
            "train_data (68530, 101, 5)\n",
            "val_data (9330, 101, 5)\n",
            "train_data (68530, 101, 5)\n",
            "val_data (10693, 101, 5)\n",
            "train_data (68530, 101, 5)\n",
            "val_data (10993, 101, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMox6wKFRU7x"
      },
      "source": [
        "def true_positive(sparse_yTrue, sparse_yPred, class_label):\n",
        "  y_true = K.zeros(len(sparse_yTrue), 1)\n",
        "  y_true = tf.where(sparse_yTrue == class_label, 1, y_true)\n",
        "  y_pred = K.zeros(len(sparse_yPred), 1)\n",
        "  y_pred= tf.where(sparse_yPred == class_label, 1, y_pred)\n",
        "\n",
        "  # print(\"sparse_yPred\", sparse_yPred)\n",
        "  # print(\"y_pred\", y_pred)\n",
        "  tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) # TP\n",
        "  # y_pos = K.round(K.clip(y_true, 0, 1))\n",
        "  # n_pos = K.sum(y_pos)\n",
        "  # y_neg = 1 - y_pos\n",
        "  # n_neg = K.sum(y_neg)\n",
        "  # n = n_pos + n_neg\n",
        "  return tp\n",
        "\n",
        "def true_negative(sparse_yTrue, sparse_yPred, class_label):\n",
        "  y_true = K.zeros(len(sparse_yTrue), 1)\n",
        "  y_true = tf.where(sparse_yTrue == class_label, 1, y_true)\n",
        "  y_pred = K.zeros(len(sparse_yPred), 1)\n",
        "  y_pred= tf.where(sparse_yPred == class_label, 1, y_pred)\n",
        "\n",
        "  y_pos = K.round(K.clip(y_true, 0, 1))\n",
        "  # n_pos = K.sum(y_pos)\n",
        "  y_neg = tf.convert_to_tensor(1 - y_pos)\n",
        "  # n_neg = K.sum(y_neg)\n",
        "  # n = n_pos + n_neg\n",
        "  y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
        "  y_pred_neg = tf.convert_to_tensor(1 - y_pred_pos)\n",
        "  tn = K.sum(K.round(K.clip(y_neg * y_pred_neg, 0, 1))) # TN\n",
        "  return tn\n",
        "\n",
        "def false_positive(sparse_yTrue, sparse_yPred, class_label):\n",
        "  y_true = K.zeros(len(sparse_yTrue), 1)\n",
        "  y_true = tf.where(sparse_yTrue == class_label, 1, y_true)\n",
        "  y_pred = K.zeros(len(sparse_yPred), 1)\n",
        "  y_pred= tf.where(sparse_yPred == class_label, 1, y_pred)\n",
        "\n",
        "  y_pos = K.round(K.clip(y_true, 0, 1))\n",
        "  # n_pos = K.sum(y_pos)\n",
        "  y_neg = tf.convert_to_tensor(1 - y_pos)\n",
        "  # n_neg = K.sum(y_neg)\n",
        "  # n = n_pos + n_neg\n",
        "  fp = K.sum(K.round(K.clip(y_neg * y_pred, 0, 1))) # FP\n",
        "  return fp\n",
        "\n",
        "def false_negative(sparse_yTrue, sparse_yPred, class_label):\n",
        "  # print(len(sparse_yTrue))\n",
        "  y_true = K.zeros(len(sparse_yTrue), 1)\n",
        "  y_true = tf.where(sparse_yTrue == class_label, 1, y_true)\n",
        "  y_pred = K.zeros(len(sparse_yPred), 1)\n",
        "  y_pred= tf.where(sparse_yPred == class_label, 1, y_pred)\n",
        "\n",
        "  y_pos = K.round(K.clip(y_true, 0, 1))\n",
        "  # n_pos = K.sum(y_pos)\n",
        "  y_neg = tf.convert_to_tensor(1 - y_pos)\n",
        "  # n_neg = K.sum(y_neg)\n",
        "  # n = n_pos + n_neg\n",
        "  y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
        "  y_pred_neg = tf.convert_to_tensor(1 - y_pred_pos)\n",
        "  fn = K.sum(K.round(K.clip(y_true * y_pred_neg, 0, 1))) # FN\n",
        "  return fn\n",
        "\n",
        "# def true_positive(sparse_yTrue, sparse_yPred, class_label):\n",
        "#   y_true = K.zeros(len(sparse_yTrue), 1)\n",
        "#   y_true = tf.where(sparse_yTrue == class_label, y_true, 1)\n",
        "#   y_pred = K.zeros(len(sparse_yPred), 1)\n",
        "#   y_pred= tf.where(sparse_yPred == class_label, y_pred, 1)\n",
        "#   tp = 0\n",
        "#   for i in tf.range(len(y_pred)):\n",
        "#     if y_true[i] == y_pred[i] == 1:\n",
        "#       tp += 1\n",
        "#   tp = tf.convert_to_tensor(tp)\n",
        "#   # tp = tf.cast(tp, dtype=float64)\n",
        "\n",
        "#   tp1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) # TP\n",
        "#   print(\"tp\", tp)\n",
        "#   print(\"tp1\", tp1)\n",
        "\n",
        "#   return tp\n",
        "\n",
        "# def true_negative(sparse_yTrue, sparse_yPred, class_label):\n",
        "#   y_true = K.zeros(len(sparse_yTrue), 1)\n",
        "#   y_true = tf.where(sparse_yTrue == class_label, y_true, 1)\n",
        "#   y_pred = K.zeros(len(sparse_yPred), 1)\n",
        "#   y_pred= tf.where(sparse_yPred == class_label, y_pred, 1)\n",
        "#   tn = 0\n",
        "#   for i in tf.range(len(y_pred)):\n",
        "#     if y_true[i] == y_pred[i] == 0:\n",
        "#       tn += 1\n",
        "#   tn = tf.convert_to_tensor(tn)\n",
        "\n",
        "#   y_pos = K.round(K.clip(y_true, 0, 1))\n",
        "#   y_neg = tf.convert_to_tensor(1 - y_pos)\n",
        "#   y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
        "#   y_pred_neg = tf.convert_to_tensor(1 - y_pred_pos)\n",
        "#   tn1 = K.sum(K.round(K.clip(y_neg * y_pred_neg, 0, 1))) # TN\n",
        "\n",
        "#   print(\"tn\", tn)\n",
        "#   print(\"tn1\", tn1)\n",
        "#   return tn\n",
        "\n",
        "# def false_positive(sparse_yTrue, sparse_yPred, class_label):\n",
        "#   y_true = K.zeros(len(sparse_yTrue), 1)\n",
        "#   y_true = tf.where(sparse_yTrue == class_label, y_true, 1)\n",
        "#   y_pred = K.zeros(len(sparse_yPred), 1)\n",
        "#   y_pred= tf.where(sparse_yPred == class_label, y_pred, 1)\n",
        "#   fp = 0\n",
        "#   for i in tf.range(len(y_pred)):\n",
        "#     if y_pred[i] == 1 and y_true[i] != y_pred[i]:\n",
        "#       fp += 1\n",
        "#   fp = tf.convert_to_tensor(fp)\n",
        "\n",
        "#   y_pos = K.round(K.clip(y_true, 0, 1))\n",
        "#   y_neg = tf.convert_to_tensor(1 - y_pos)\n",
        "#   fp1 = K.sum(K.round(K.clip(y_neg * y_pred, 0, 1))) # FP\n",
        "\n",
        "#   print(\"fp\", fp)\n",
        "#   print(\"fp1\", fp1)\n",
        "#   return fp\n",
        "\n",
        "# def false_negative(sparse_yTrue, sparse_yPred, class_label):\n",
        "#   # print(len(sparse_yTrue))\n",
        "#   y_true = K.zeros(len(sparse_yTrue), 1)\n",
        "#   y_true = tf.where(sparse_yTrue == class_label, y_true, 1)\n",
        "#   y_pred = K.zeros(len(sparse_yPred), 1)\n",
        "#   y_pred= tf.where(sparse_yPred == class_label, y_pred, 1)\n",
        "#   fn = 0\n",
        "#   for i in tf.range(len(y_pred)):\n",
        "#     if y_pred[i] == 0 and y_true[i] != y_pred[i]:\n",
        "#       fn += 1\n",
        "#   fn = tf.convert_to_tensor(fn)\n",
        "\n",
        "#   y_pos = K.round(K.clip(y_true, 0, 1))\n",
        "#   y_neg = tf.convert_to_tensor(1 - y_pos)\n",
        "#   y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
        "#   y_pred_neg = tf.convert_to_tensor(1 - y_pred_pos)\n",
        "#   fn1 = K.sum(K.round(K.clip(y_true * y_pred_neg, 0, 1))) # FN\n",
        "  \n",
        "#   print(\"fn\", fn)\n",
        "#   print(\"fn1\", fn1)\n",
        "#   return fn\n",
        "\n",
        "\n",
        "def values(sparse_yTrue, sparse_yPred, class_label):\n",
        "  TP = true_positive(sparse_yPred, sparse_yTrue, class_label)\n",
        "  TP = K.cast(TP, dtype='float64')\n",
        "  TN = true_negative(sparse_yPred, sparse_yTrue, class_label)\n",
        "  TN = K.cast(TN, dtype='float64')\n",
        "  FP = false_positive(sparse_yPred, sparse_yTrue, class_label)\n",
        "  FP = K.cast(FP, dtype='float64')\n",
        "  FN = false_negative(sparse_yPred, sparse_yTrue, class_label)\n",
        "  FN = K.cast(FN, dtype='float64')\n",
        "  # print(\"TP\", TP)\n",
        "  # print(\"FN\", FN)\n",
        "  # print(\"TN\", TN)\n",
        "  # print(\"FP\", FP)\n",
        "  # print(\"1/(TP+FN)\", 1/(TP+FN))             # infになる、TP+FN = 0になるとき\n",
        "  # print(\"1/(TN+FP)\", 1/(TN+FP))             # infになる、TN+FP = 0になるとき\n",
        "  # print(\"tf.convert_to_tensor( TP * 1/(TP+FN) * FP * 1/(TN+FP) )\", tf.convert_to_tensor( TP * 1/(TP+FN) * FP * 1/(TN+FP) ))\n",
        "  geometric_value = K.square( tf.convert_to_tensor( TP * 1/(TP+FN) * FP * 1/(TN+FP) ) )\n",
        "  # print(\"geometric_value\", geometric_value)     # infによってNanになる\n",
        "  accuracy = tf.convert_to_tensor( (TP + TN) / (TP + TN + FP + FN) )\n",
        "  return K.cast(tf.convert_to_tensor(geometric_value), dtype='float64'), K.cast(tf.convert_to_tensor(accuracy), dtype='float64')"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRfk1owgGbHQ"
      },
      "source": [
        "# from keras import backend as K\n",
        "\n",
        "# def customLoss(yTrue, yPred):\n",
        "#   # print(\"yTrue.shape\", yTrue.shape)\n",
        "#   # print(\"yPred.shape\", yPred.shape)\n",
        "#   print(\"yTrue\", yTrue)\n",
        "#   print(\"yPred\", yPred)\n",
        "#   sparse_yTrue = K.argmax(yTrue, axis=1)           # (50, 1)\n",
        "#   sparse_yPred = K.argmax(yPred, axis=1)           # (50, 1)\n",
        "  \n",
        "#   # from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
        "  \n",
        "# #-------あっているはず-----------------------------------------------------------------------------------------------------------------------\n",
        "#   HC_num = tf.cast(tf.convert_to_tensor(len(sparse_yTrue[sparse_yTrue==0])), dtype=tf.float64)\n",
        "#   # print(\"HC_num\", HC_num)\n",
        "#   H_num = tf.cast(tf.convert_to_tensor(len(sparse_yTrue[sparse_yTrue==1])), dtype=tf.float64)\n",
        "#   K_num = tf.cast(tf.convert_to_tensor(len(sparse_yTrue[sparse_yTrue==2])), dtype=tf.float64)\n",
        "#   A_num = tf.cast(tf.convert_to_tensor(len(sparse_yTrue[sparse_yTrue==3])), dtype=tf.float64)\n",
        "#   C_num = tf.cast(tf.convert_to_tensor(len(sparse_yTrue[sparse_yTrue==4])), dtype=tf.float64)\n",
        "\n",
        "#   HC_IR = tf.convert_to_tensor(HC_num/len(sparse_yTrue[sparse_yTrue!=0]))\n",
        "#   HC_IR = tf.cast(HC_IR, dtype=tf.float64)\n",
        "#   H_IR = tf.convert_to_tensor(H_num/len(sparse_yTrue[sparse_yTrue!=1]))\n",
        "#   H_IR = tf.cast(H_IR, dtype=tf.float64)\n",
        "#   K_IR = tf.convert_to_tensor(K_num/len(sparse_yTrue[sparse_yTrue!=2]))\n",
        "#   K_IR = tf.cast(K_IR, dtype=tf.float64)\n",
        "#   A_IR = tf.convert_to_tensor(A_num/len(sparse_yTrue[sparse_yTrue!=3]))\n",
        "#   A_IR = tf.cast(A_IR, dtype=tf.float64)\n",
        "#   C_IR = tf.convert_to_tensor(C_num/len(sparse_yTrue[sparse_yTrue!=4]))\n",
        "#   C_IR = tf.cast(C_IR, dtype=tf.float64)\n",
        "# #-------あっているはず-----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "#   # print(\"HC_IR\", HC_IR)          # Nanなし\n",
        "#   # print(\"H_IR\", H_IR)\n",
        "#   # print(\"K_IR\", K_IR)\n",
        "#   # print(\"A_IR\", A_IR)\n",
        "#   # print(\"C_IR\", C_IR)\n",
        "\n",
        "#   # print(\"sparse_yPred_prob\", sparse_yPred_prob)\n",
        "#   HC_idx = tf.where(sparse_yTrue==0)\n",
        "#   H_idx = tf.where(sparse_yTrue==1)\n",
        "#   K_idx = tf.where(sparse_yTrue==2)\n",
        "#   A_idx = tf.where(sparse_yTrue==3)\n",
        "#   C_idx = tf.where(sparse_yTrue==4)\n",
        "#   # print(\"A_idx = tf.where(sparse_yTrue==3)[0]\", A_idx)\n",
        "  \n",
        "#   HC_sparse_yPred_prob = tf.gather(yPred, HC_idx)\n",
        "#   H_sparse_yPred_prob = tf.gather(yPred, H_idx)\n",
        "#   K_sparse_yPred_prob = tf.gather(yPred, K_idx)\n",
        "#   A_sparse_yPred_prob = tf.gather(yPred, A_idx)\n",
        "#   C_sparse_yPred_prob = tf.gather(yPred, C_idx)\n",
        "\n",
        "#   # print(\"HC_sparse_yPred_prob\", HC_sparse_yPred_prob)          # Nanなし\n",
        "#   # print(\"H_sparse_yPred_prob\", H_sparse_yPred_prob)\n",
        "#   # print(\"K_sparse_yPred_prob\", K_sparse_yPred_prob)\n",
        "#   # print(\"A_sparse_yPred_prob\", A_sparse_yPred_prob)\n",
        "#   # print(\"C_sparse_yPred_prob\", C_sparse_yPred_prob)\n",
        "\n",
        "#   HC_sparse_yPred_prob = tf.gather(HC_sparse_yPred_prob, indices=0, axis=-1)\n",
        "#   H_sparse_yPred_prob = tf.gather(H_sparse_yPred_prob, indices=1, axis=-1)\n",
        "#   K_sparse_yPred_prob = tf.gather(K_sparse_yPred_prob, indices=2, axis=-1)\n",
        "#   A_sparse_yPred_prob = tf.gather(A_sparse_yPred_prob, indices=3, axis=-1)\n",
        "#   C_sparse_yPred_prob = tf.gather(C_sparse_yPred_prob, indices=4, axis=-1)\n",
        "\n",
        "#   # print(\"HC_sparse_yPred_prob\", HC_sparse_yPred_prob)          # Nanなし\n",
        "#   # print(\"H_sparse_yPred_prob\", H_sparse_yPred_prob)\n",
        "#   # print(\"K_sparse_yPred_prob\", K_sparse_yPred_prob)\n",
        "#   # print(\"A_sparse_yPred_prob\", A_sparse_yPred_prob)\n",
        "#   # print(\"C_sparse_yPred_prob\", C_sparse_yPred_prob)\n",
        "\n",
        "#   HC_g_values, HC_acc = values(sparse_yTrue, sparse_yPred, 0)\n",
        "#   H_g_values, H_acc = values(sparse_yTrue, sparse_yPred, 1)\n",
        "#   K_g_values, K_acc = values(sparse_yTrue, sparse_yPred, 2)\n",
        "#   A_g_values, A_acc = values(sparse_yTrue, sparse_yPred, 3)\n",
        "#   C_g_values, C_acc = values(sparse_yTrue, sparse_yPred, 4)\n",
        "\n",
        "#   # print(\"HC_g_values\", HC_g_values)           # Nanある\n",
        "#   # print(\"H_g_values\", H_g_values)\n",
        "#   # print(\"K_g_values\", K_g_values)\n",
        "#   # print(\"A_g_values\", A_g_values)\n",
        "#   # print(\"C_g_values\", C_g_values)\n",
        "\n",
        "#   # print(\"HC_acc\", HC_acc)                     # Nanある\n",
        "#   # print(\"H_acc\", H_acc)\n",
        "#   # print(\"K_acc\", K_acc)\n",
        "#   # print(\"A_acc\", A_acc)\n",
        "#   # print(\"C_acc\", C_acc)\n",
        "\n",
        "#   HC_mis_cost = tf.cast(tf.convert_to_tensor( HC_IR * K.exp(- HC_g_values / 2) * K.exp(- HC_acc / 2)), dtype=tf.float64)\n",
        "#   H_mis_cost =  tf.cast(tf.convert_to_tensor( H_IR * K.exp(- H_g_values / 2) * K.exp(- H_acc / 2)), dtype=tf.float64)\n",
        "#   K_mis_cost = tf.cast(tf.convert_to_tensor( K_IR * K.exp(- K_g_values / 2) * K.exp(- K_acc / 2)), dtype=tf.float64)\n",
        "#   A_mis_cost = tf.cast(tf.convert_to_tensor(A_IR * K.exp(- A_g_values / 2) * K.exp(- A_acc / 2)), dtype=tf.float64)\n",
        "#   C_mis_cost = tf.cast(tf.convert_to_tensor( C_IR * K.exp(- C_g_values / 2) * K.exp(- C_acc / 2)), dtype=tf.float64)\n",
        "\n",
        "#   # HC_loss = tf.cast(tf.convert_to_tensor(  HC_mis_cost * 1/HC_num * -1 * tf.cast(K.log(HC_sparse_yPred_prob), dtype=tf.float64)), dtype=tf.float64)\n",
        "#   # H_loss = tf.cast(tf.convert_to_tensor(  H_mis_cost * 1/H_num * -1 * tf.cast(K.log(H_sparse_yPred_prob), dtype=tf.float64)), dtype=tf.float64)\n",
        "#   # K_loss = tf.cast(tf.convert_to_tensor( K_mis_cost * 1/K_num * -1 * tf.cast(K.log(K_sparse_yPred_prob), dtype=tf.float64)), dtype=tf.float64)\n",
        "#   # A_loss = tf.cast(tf.convert_to_tensor( A_mis_cost * 1/A_num * -1 * tf.cast(K.log(A_sparse_yPred_prob), dtype=tf.float64)), dtype=tf.float64)\n",
        "#   # C_loss = tf.cast(tf.convert_to_tensor( C_mis_cost * 1/C_num * -1 * tf.cast(K.log(C_sparse_yPred_prob), dtype=tf.float64)), dtype=tf.float64)\n",
        "  \n",
        "#   # HC_loss = tf.cast(tf.convert_to_tensor(  HC_mis_cost *  -1 * tf.cast(K.log(HC_sparse_yPred_prob), dtype=tf.float64)), dtype=tf.float64)\n",
        "#   # H_loss = tf.cast(tf.convert_to_tensor(  H_mis_cost *  -1 * tf.cast(K.log(H_sparse_yPred_prob), dtype=tf.float64)), dtype=tf.float64)\n",
        "#   # K_loss = tf.cast(tf.convert_to_tensor( K_mis_cost * -1 * tf.cast(K.log(K_sparse_yPred_prob), dtype=tf.float64)), dtype=tf.float64)\n",
        "#   # A_loss = tf.cast(tf.convert_to_tensor( A_mis_cost *  -1 * tf.cast(K.log(A_sparse_yPred_prob), dtype=tf.float64)), dtype=tf.float64)\n",
        "#   # C_loss = tf.cast(tf.convert_to_tensor( C_mis_cost *  -1 * tf.cast(K.log(C_sparse_yPred_prob), dtype=tf.float64)), dtype=tf.float64\n",
        "  \n",
        "#   HC_loss = tf.cast(tf.convert_to_tensor(  -1 * tf.cast(tf.math.log(HC_sparse_yPred_prob), dtype=tf.float64)), dtype=tf.float64)\n",
        "#   H_loss = tf.cast(tf.convert_to_tensor(  -1 * tf.cast(tf.math.log(H_sparse_yPred_prob), dtype=tf.float64)), dtype=tf.float64)\n",
        "#   K_loss = tf.cast(tf.convert_to_tensor( -1 * tf.cast(tf.math.log(K_sparse_yPred_prob), dtype=tf.float64)), dtype=tf.float64)\n",
        "#   A_loss = tf.cast(tf.convert_to_tensor( -1 * tf.cast(tf.math.log(A_sparse_yPred_prob), dtype=tf.float64)), dtype=tf.float64)\n",
        "#   C_loss = tf.cast(tf.convert_to_tensor( -1 * tf.cast(tf.math.log(C_sparse_yPred_prob), dtype=tf.float64)), dtype=tf.float64)\n",
        "\n",
        "#   # print(\"HC_sparse_yPred_prob\", HC_sparse_yPred_prob)          #Nanなし\n",
        "#   # print(\"H_sparse_yPred_prob\", H_sparse_yPred_prob)\n",
        "#   # print(\"K_sparse_yPred_prob\", K_sparse_yPred_prob)\n",
        "#   # print(\"A_sparse_yPred_prob\", A_sparse_yPred_prob)\n",
        "#   # print(\"C_sparse_yPred_prob\", C_sparse_yPred_prob)\n",
        "\n",
        "#   # print(\"HC_mis_cost\",HC_mis_cost)\n",
        "#   # print(\"H_mis_cost\",H_mis_cost)\n",
        "#   # print(\"K_mis_cost\",K_mis_cost)\n",
        "#   # print(\"A_mis_cost\",A_mis_cost)\n",
        "#   # print(\"C_mis_cost\",C_mis_cost)\n",
        "\n",
        "#   # print(\"HC_num\", HC_num)\n",
        "#   # print(\"H_num\", H_num)\n",
        "#   # print(\"K_num\", K_num)\n",
        "#   # print(\"A_num\", A_num)\n",
        "#   # print(\"C_num\", C_num)\n",
        "\n",
        "#   # print(HC_loss)\n",
        "#   # print(H_loss)\n",
        "#   overall_loss = K.concatenate([HC_loss, H_loss, K_loss, A_loss, C_loss], axis=0)\n",
        "#   # overall_loss = tf.convert_to_tensor(overall_loss * 10)\n",
        "#   overall_loss = tf.cast(overall_loss, dtype=tf.float64)\n",
        "#   print(\"overall_loss\", overall_loss)\n",
        "#   return overall_loss"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWXZstbIc53D"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def miscost(yTrue, yPred):\n",
        "  # print(\"yTrue.shape\", yTrue.shape)\n",
        "  # print(\"yPred.shape\", yPred.shape)\n",
        "  # print(\"yTrue\", yTrue)\n",
        "  # print(\"yPred\", yPred)\n",
        "  sparse_yTrue = K.argmax(yTrue, axis=1)           # (50, 1)\n",
        "  sparse_yPred = K.argmax(yPred, axis=1)           # (50, 1)\n",
        "  \n",
        "  HC_num = K.cast(tf.convert_to_tensor(len(sparse_yTrue[sparse_yTrue==0])), dtype='float64')\n",
        "  H_num = K.cast(tf.convert_to_tensor(len(sparse_yTrue[sparse_yTrue==1])), dtype='float64')\n",
        "  K_num = K.cast(tf.convert_to_tensor(len(sparse_yTrue[sparse_yTrue==2])), dtype='float64')\n",
        "  A_num = K.cast(tf.convert_to_tensor(len(sparse_yTrue[sparse_yTrue==3])), dtype='float64')\n",
        "  C_num = K.cast(tf.convert_to_tensor(len(sparse_yTrue[sparse_yTrue==4])), dtype='float64')\n",
        "\n",
        "  HC_IR = tf.convert_to_tensor(HC_num/K.cast(len(sparse_yTrue[sparse_yTrue!=0]), dtype='float64'))\n",
        "  # HC_IR = tf.cast(HC_IR, dtype=tf.float64)\n",
        "  H_IR = tf.convert_to_tensor(H_num/K.cast(len(sparse_yTrue[sparse_yTrue!=1]), dtype='float64'))\n",
        "  # H_IR = tf.cast(H_IR, dtype=tf.float64)\n",
        "  K_IR = tf.convert_to_tensor(K_num/K.cast(len(sparse_yTrue[sparse_yTrue!=2]), dtype='float64'))\n",
        "  # K_IR = tf.cast(K_IR, dtype=tf.float64)\n",
        "  A_IR = tf.convert_to_tensor(A_num/K.cast(len(sparse_yTrue[sparse_yTrue!=3]), dtype='float64'))\n",
        "  # A_IR = tf.cast(A_IR, dtype=tf.float64)\n",
        "  C_IR = tf.convert_to_tensor(C_num/K.cast(len(sparse_yTrue[sparse_yTrue!=4]), dtype='float64'))\n",
        "  # C_IR = tf.cast(C_IR, dtype=tf.float64)\n",
        "\n",
        "  HC_g_values, HC_acc = values(sparse_yTrue, sparse_yPred, 0)\n",
        "  H_g_values, H_acc = values(sparse_yTrue, sparse_yPred, 1)\n",
        "  K_g_values, K_acc = values(sparse_yTrue, sparse_yPred, 2)\n",
        "  A_g_values, A_acc = values(sparse_yTrue, sparse_yPred, 3)\n",
        "  C_g_values, C_acc = values(sparse_yTrue, sparse_yPred, 4)\n",
        "\n",
        "  HC_mis_cost = K.cast(tf.convert_to_tensor( HC_IR * K.exp(- HC_g_values / 2) * K.exp(- HC_acc / 2)), dtype='float64')\n",
        "  H_mis_cost =  K.cast(tf.convert_to_tensor( H_IR * K.exp(- H_g_values / 2) * K.exp(- H_acc / 2)), dtype='float64')\n",
        "  K_mis_cost = K.cast(tf.convert_to_tensor( K_IR * K.exp(- K_g_values / 2) * K.exp(- K_acc / 2)), dtype='float64')\n",
        "  A_mis_cost = K.cast(tf.convert_to_tensor(A_IR * K.exp(- A_g_values / 2) * K.exp(- A_acc / 2)), dtype='float64')\n",
        "  C_mis_cost = K.cast(tf.convert_to_tensor( C_IR * K.exp(- C_g_values / 2) * K.exp(- C_acc / 2)), dtype='float64')\n",
        "  return HC_mis_cost, H_mis_cost, K_mis_cost, A_mis_cost, C_mis_cost, sparse_yTrue"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCTYw7M9IuUd"
      },
      "source": [
        "from keras import backend as K\n",
        "def customLoss(output, target, from_logits=False):\n",
        "  _EPSILON = K.epsilon()\n",
        "  # print(_EPSILON)\n",
        "  # scale preds so that the class probas of each sample sum to 1\n",
        "  output /= tf.math.reduce_sum(output,\n",
        "                          # reduction_indices=len(output.get_shape()) - 1,\n",
        "                          axis=-1,\n",
        "                          keepdims=True)\n",
        "  output = K.cast(output, dtype='float64')\n",
        "  # print(output)\n",
        "  epsilon = tf.convert_to_tensor(_EPSILON)\n",
        "  epsilon = K.cast(epsilon, dtype='float64')\n",
        "  output = tf.clip_by_value(output, epsilon, 1. - epsilon)\n",
        "  # print(target)\n",
        "  # print(output)\n",
        "  target = K.cast(target, dtype='float64')\n",
        "  # print(target)\n",
        "  loss = - tf.math.reduce_sum(target * K.log(output),\n",
        "                        #  reduction_indices=len(output.get_shape()) - 1\n",
        "                        axis=-1\n",
        "                          )\n",
        "  # HC_mis_cost, H_mis_cost, K_mis_cost, A_mis_cost, C_mis_cost, sparse_yTrue = miscost(output, target)\n",
        "  # loss_1 = tf.where(sparse_yTrue == 0, tf.convert_to_tensor(loss*HC_mis_cost), loss)\n",
        "  # loss_1 = tf.where(sparse_yTrue == 1, tf.convert_to_tensor(loss*H_mis_cost), loss_1)\n",
        "  # loss_1 = tf.where(sparse_yTrue == 2, tf.convert_to_tensor(loss*K_mis_cost), loss_1)\n",
        "  # loss_1 = tf.where(sparse_yTrue == 3, tf.convert_to_tensor(loss*A_mis_cost), loss_1)\n",
        "  # loss_1 = tf.where(sparse_yTrue == 4, tf.convert_to_tensor(loss*C_mis_cost), loss_1)\n",
        "  # print(loss)\n",
        "  # print(H_mis_cost)\n",
        "  # print(sparse_yTrue)\n",
        "  # print(loss_1)\n",
        "  # return loss_1\n",
        "  return loss"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLSQrnXt7sHf"
      },
      "source": [
        "tf.config.run_functions_eagerly(True)\n",
        "# tf.config.run_functions_eagerly(False)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUu_-ywQrpsL"
      },
      "source": [
        "for i in range(0, 5):\n",
        "  # print(\"HC\", np.where(np.argmax(train_label_list[i], axis=1)==0)[0].shape)\n",
        "  # print(\"H\", np.where(np.argmax(train_label_list[i], axis=1)==1)[0].shape)\n",
        "  # print(\"K\", np.where(np.argmax(train_label_list[i], axis=1)==2)[0].shape)\n",
        "  # print(\"A\", np.where(np.argmax(train_label_list[i], axis=1)==3)[0].shape)\n",
        "  # print(\"C\", np.where(np.argmax(train_label_list[i], axis=1)==4)[0].shape)\n",
        "\n",
        "  model = build_model()\n",
        "  model.compile(\n",
        "      loss=customLoss,\n",
        "      # loss = \"categorical_crossentropy\",\n",
        "      optimizer= optimizers.Adam(lr=1e-5, beta_1= 0.9, beta_2= 0.999),\n",
        "      metrics=['acc'],\n",
        "      # class_weight=class_weight\n",
        "            )\n",
        "\n",
        "  result = model.fit(\n",
        "      x=train_LSTM_data_list[i], \n",
        "      y=train_label_list[i], \n",
        "      batch_size=BATCH_SIZE, \n",
        "      epochs=400, \n",
        "      verbose=1,\n",
        "      validation_data=(\n",
        "          val_LSTM_data_list[i], \n",
        "          val_label_list[i]\n",
        "            ),\n",
        "      callbacks=[early_stopping]\n",
        "      )\n",
        "  \n",
        "  print(\"results per {}-fold\".format(i))\n",
        "  history.append(model.evaluate(\n",
        "      x=val_LSTM_data_list[i], \n",
        "      y=val_label_list[i], \n",
        "      verbose=1))\n",
        "  # save weights\n",
        "  # now = datetime.datetime.now()\n",
        "  # file_name = \"drive/MyDrive/saved_models/1DCNN_8class_with_both/{}fold_{}_{}\".format(i, NAME, now.strftime('%Y%m%d_%H%M%S'))\n",
        "  file_name = \"drive/MyDrive/saved_models/StratifiedGroupFold/COP_LSTM_5class_with_both/{}fold_{}\".format(i, NAME)\n",
        "  model.save(file_name+'.h5')\n",
        "\n",
        "    # 混合行列\n",
        "  predict_prob = model.predict(val_LSTM_data_list[i])\n",
        "  predict_classes=np.argmax(predict_prob,axis=1)\n",
        "  true_classes = val_label_list[i]\n",
        "  true_classes=np.argmax(true_classes, axis=1)\n",
        "  cmx = confusion_matrix(true_classes, predict_classes)\n",
        "  cmx_list.append(cmx)\n",
        "  print(cmx)\n",
        "\n",
        "  index = [\"HC\", \"a_H\", \"a_K\", \"a_A\", \"a_C\"]\n",
        "\n",
        "  report = classification_report(true_classes, predict_classes, target_names=index)\n",
        "  report_list.append(report)\n",
        "  print(report)\n",
        "\n",
        "  cmx_data = []\n",
        "  for i in range(len(cmx)):\n",
        "    for j in range(len(cmx)): \n",
        "      cmx_data.append((cmx[j, i] / np.sum(cmx[i])))\n",
        "\n",
        "  cmx_data = np.array(cmx_data).reshape(classes_main, classes_main)\n",
        "\n",
        "  ax= plt.subplot()\n",
        "  sns.heatmap(cmx_data, annot=True, ax = ax, fmt=\"1.3f\", cmap=\"Blues\")\n",
        "  ax.set_xlabel('Predicted labels')\n",
        "  ax.set_ylabel('True labels')\n",
        "  ax.set_title('Confusion Matrix') \n",
        "  ax.xaxis.set_ticklabels(index) \n",
        "  ax.yaxis.set_ticklabels(index)\n",
        "  plt.show()\n",
        "\n",
        "  i = i + 1\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(result.history['acc'])\n",
        "plt.plot(result.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.grid()\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(result.history['loss'])\n",
        "plt.plot(result.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.grid()\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "with open(\"/content/drive/MyDrive/saved_models/StratifiedGroupFold/COP_LSTM_5class_with_both/cmx_{}_{}.pickle\".format(NAME, now.strftime('%Y%m%d_%H%M%S')), mode=\"wb\") as f:\n",
        "   pickle.dump(cmx_list, f)\n",
        "f.close()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/saved_models/StratifiedGroupFold/COP_LSTM_5class_with_both/classification_report_{}_{}.pickle\".format(NAME, now.strftime('%Y%m%d_%H%M%S')), mode=\"wb\") as f:\n",
        "   pickle.dump(report_list, f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr40754Ban4N"
      },
      "source": [
        "# min_max normalizationする場合\n",
        "from sklearn import preprocessing\n",
        "mmscaler_LSTM = preprocessing.MinMaxScaler(feature_range=(-1, 1), copy=True) # インスタンスの作成、活性化関数でtanhを使うのでfeature_range=(-1,1) \n",
        "# mmscaler_CNN = preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True) # インスタンスの作成、活性化関数でtanhを使うのでfeature_range=(-1,1)\n",
        "\n",
        "# LSTM\n",
        "mmscaler_LSTM.fit(test_V)\n",
        "test_V = mmscaler_LSTM.transform(test_V)\n",
        "test_V = test_V.reshape(test_V.shape[0], test_V.shape[1], 1)\n",
        "\n",
        "mmscaler_LSTM.fit(test_AP)\n",
        "test_AP = mmscaler_LSTM.transform(test_AP)\n",
        "test_AP = test_AP.reshape(test_AP.shape[0], test_AP.shape[1], 1)\n",
        "\n",
        "mmscaler_LSTM.fit(test_ML)\n",
        "test_ML = mmscaler_LSTM.transform(test_ML)\n",
        "test_ML = test_ML.reshape(test_ML.shape[0], test_ML.shape[1], 1)\n",
        "\n",
        "mmscaler_LSTM.fit(test_COP_AP)\n",
        "test_COP_AP = mmscaler_LSTM.transform(test_COP_AP)\n",
        "test_COP_AP = test_COP_AP.reshape(test_COP_AP.shape[0], test_COP_AP.shape[1], 1)\n",
        "\n",
        "mmscaler_LSTM.fit(test_COP_ML)\n",
        "test_COP_ML = mmscaler_LSTM.transform(test_COP_ML)\n",
        "test_COP_ML = test_COP_ML.reshape(test_COP_ML.shape[0], test_COP_ML.shape[1], 1)\n",
        "\n",
        "test_x = np.concatenate([test_V[test_index], test_AP[test_index], test_ML[test_index], test_COP_AP[test_index], test_COP_ML[test_index]], axis=2)\n",
        "test_y = test_label[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ICuRJZiZjco"
      },
      "source": [
        "# test \n",
        "model = build_model()\n",
        "model.load_weights('/content/drive/MyDrive/saved_models/StratifiedGroupFold/COP_LSTM_5class_with_both/0fold_COP_LSTM_5class_stratified_group_with_both.h5')\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "          optimizer= optimizers.Adam(lr=1e-5, beta_1= 0.9, beta_2= 0.999),\n",
        "          metrics=['accuracy']\n",
        "          )\n",
        "loss, acc = model.evaluate(\n",
        "    x=test_x, \n",
        "    y=test_y,\n",
        "    verbose=1)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}